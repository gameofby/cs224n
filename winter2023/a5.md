# 1. Attention exploration

## (a) Copying in attention

### i. (1 point) Explain why $\alpha$ can be interpreted as a categorical probability distribution.

Because the formula of $\alpha$ have the same properties as the Categorical Distribution
- Non-negative probability: the range of $\alpha$ is larger than zero
- Sum to one: $\sum_{i=1}^{n} \alpha_i = 1$
- Discrete Categories: there is a finite number of discrete categories, which equals finite number of attention heads n

### ii. (2 points) The distribution $ \alpha $ is typically relatively "diffuse"; the probability mass is spread out between many different $ \alpha_i $. However, this is not always the case. Describe (in one sentence) under what conditions the categorical distribution $ \alpha $ puts almost all of its weight on some $ \alpha_j $, where $ j \in \{1, \ldots, n\} $ (i.e. $ \alpha_j \gg \sum_{i \neq j} \alpha_i $). What must be true about the query $ q $ and/or the $\{k_1, \ldots, k_n\}$ ?

The vector $q_i$ and vector $k_i$ of $\alpha_j$ must be non-orthogonal, others are orthogonal.

### iii. (1 point) Under the conditions you gave in (ii), **describe** the output $c$.
$c$ will approximately equal to $v_j$, as $\alpha_j$ will approximately equal to 1


### iv.(1 point) Explain (in two sentences or fewer) what your answer to (ii) and (iii) means intuitively.

After pretraining, the model learned that the word of the query vector has strong relationship with the word that gives the largest $k_j$

